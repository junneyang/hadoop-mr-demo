[DEBUG] 2016-09-07 11:25:13.228 [] [] [main] com.ares.hadoop.mr.wordcount.MRTest.main(MRTest.java:18) MRTest: MRTest STARTED...
[DEBUG] 2016-09-07 11:25:13.700 [] [] [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42) field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)], about=, type=DEFAULT, always=false, sampleName=Ops)
[DEBUG] 2016-09-07 11:25:13.712 [] [] [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42) field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)], about=, type=DEFAULT, always=false, sampleName=Ops)
[DEBUG] 2016-09-07 11:25:13.713 [] [] [main] org.apache.hadoop.metrics2.lib.MutableMetricsFactory.newForField(MutableMetricsFactory.java:42) field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(valueName=Time, value=[GetGroups], about=, type=DEFAULT, always=false, sampleName=Ops)
[DEBUG] 2016-09-07 11:25:13.714 [] [] [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:221) UgiMetrics, User and group related metrics
[DEBUG] 2016-09-07 11:25:14.124 [] [] [main] org.apache.hadoop.security.authentication.util.KerberosName.<clinit>(KerberosName.java:88) Kerberos krb5 configuration not found, setting default realm to empty
[DEBUG] 2016-09-07 11:25:14.128 [] [] [main] org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:238)  Creating new Groups object
[DEBUG] 2016-09-07 11:25:14.136 [] [] [main] org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:46) Trying to load the custom-built native-hadoop library...
[DEBUG] 2016-09-07 11:25:14.145 [] [] [main] org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:55) Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[DEBUG] 2016-09-07 11:25:14.145 [] [] [main] org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:56) java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
[WARN ] 2016-09-07 11:25:14.145 [] [] [main] org.apache.hadoop.util.NativeCodeLoader.<clinit>(NativeCodeLoader.java:62) Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[DEBUG] 2016-09-07 11:25:14.146 [] [] [main] org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:40) Falling back to shell based
[DEBUG] 2016-09-07 11:25:14.150 [] [] [main] org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.<init>(JniBasedUnixGroupsMappingWithFallback.java:44) Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[DEBUG] 2016-09-07 11:25:14.195 [] [] [main] org.apache.hadoop.util.Shell.isSetsidSupported(Shell.java:396) setsid exited with exit code 0
[DEBUG] 2016-09-07 11:25:14.195 [] [] [main] org.apache.hadoop.security.Groups.<init>(Groups.java:80) Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[DEBUG] 2016-09-07 11:25:14.198 [] [] [main] org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.login(UserGroupInformation.java:195) hadoop login
[DEBUG] 2016-09-07 11:25:14.199 [] [] [main] org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:144) hadoop login commit
[DEBUG] 2016-09-07 11:25:14.202 [] [] [main] org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule.commit(UserGroupInformation.java:174) using local user:UnixPrincipal: root
[DEBUG] 2016-09-07 11:25:14.211 [] [] [main] org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:785) UGI loginUser:root (auth:SIMPLE)
[DEBUG] 2016-09-07 11:25:14.455 [] [] [main] org.apache.hadoop.hdfs.DFSClient$Conf.<init>(DFSClient.java:389) dfs.client.use.legacy.blockreader.local = false
[DEBUG] 2016-09-07 11:25:14.456 [] [] [main] org.apache.hadoop.hdfs.DFSClient$Conf.<init>(DFSClient.java:392) dfs.client.read.shortcircuit = false
[DEBUG] 2016-09-07 11:25:14.456 [] [] [main] org.apache.hadoop.hdfs.DFSClient$Conf.<init>(DFSClient.java:395) dfs.client.domain.socket.data.traffic = false
[DEBUG] 2016-09-07 11:25:14.457 [] [] [main] org.apache.hadoop.hdfs.DFSClient$Conf.<init>(DFSClient.java:398) dfs.domain.socket.path = 
[DEBUG] 2016-09-07 11:25:14.518 [] [] [main] org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:74) multipleLinearRandomRetry = null
[DEBUG] 2016-09-07 11:25:14.571 [] [] [main] org.apache.hadoop.ipc.Server.registerProtocolEngine(Server.java:228) rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcRequestWrapper, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@513ba552
[DEBUG] 2016-09-07 11:25:14.594 [] [] [main] org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63) getting client out of cache: org.apache.hadoop.ipc.Client@1f3c2480
[DEBUG] 2016-09-07 11:25:14.922 [] [] [main] org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:108) Both short-circuit local reads and UNIX domain socket are disabled.
[DEBUG] 2016-09-07 11:25:14.931 [] [] [main] org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction(UserGroupInformation.java:1638) PrivilegedAction as:root (auth:SIMPLE) from:org.apache.hadoop.mapreduce.Job.connect(Job.java:1250)
[DEBUG] 2016-09-07 11:25:14.984 [] [] [main] org.apache.hadoop.mapreduce.Cluster.initialize(Cluster.java:90) Trying ClientProtocolProvider : org.apache.hadoop.mapred.LocalClientProtocolProvider
[DEBUG] 2016-09-07 11:25:14.985 [] [] [main] org.apache.hadoop.mapreduce.Cluster.initialize(Cluster.java:108) Cannot pick org.apache.hadoop.mapred.LocalClientProtocolProvider as the ClientProtocolProvider - returned null protocol
[DEBUG] 2016-09-07 11:25:14.986 [] [] [main] org.apache.hadoop.mapreduce.Cluster.initialize(Cluster.java:90) Trying ClientProtocolProvider : org.apache.hadoop.mapred.YarnClientProtocolProvider
[DEBUG] 2016-09-07 11:25:15.030 [] [] [main] org.apache.hadoop.service.AbstractService.enterState(AbstractService.java:452) Service: org.apache.hadoop.mapred.ResourceMgrDelegate entered state INITED
[DEBUG] 2016-09-07 11:25:15.040 [] [] [main] org.apache.hadoop.service.AbstractService.enterState(AbstractService.java:452) Service: org.apache.hadoop.yarn.client.api.impl.YarnClientImpl entered state INITED
[INFO ] 2016-09-07 11:25:15.179 [] [] [main] org.apache.hadoop.yarn.client.RMProxy.createRMProxy(RMProxy.java:92) Connecting to ResourceManager at HADOOP-NODE1/10.20.0.11:8032
[DEBUG] 2016-09-07 11:25:15.183 [] [] [main] org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction(UserGroupInformation.java:1638) PrivilegedAction as:root (auth:SIMPLE) from:org.apache.hadoop.yarn.client.RMProxy.getProxy(RMProxy.java:130)
[DEBUG] 2016-09-07 11:25:15.184 [] [] [main] org.apache.hadoop.yarn.ipc.YarnRPC.create(YarnRPC.java:59) Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
[DEBUG] 2016-09-07 11:25:15.188 [] [] [main] org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getProxy(HadoopYarnProtoRPC.java:47) Creating a HadoopYarnProtoRpc proxy for protocol interface org.apache.hadoop.yarn.api.ApplicationClientProtocol
[DEBUG] 2016-09-07 11:25:15.231 [] [] [main] org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63) getting client out of cache: org.apache.hadoop.ipc.Client@1f3c2480
[DEBUG] 2016-09-07 11:25:15.411 [] [] [main] org.apache.hadoop.service.AbstractService.start(AbstractService.java:197) Service org.apache.hadoop.yarn.client.api.impl.YarnClientImpl is started
[DEBUG] 2016-09-07 11:25:15.425 [] [] [main] org.apache.hadoop.service.AbstractService.start(AbstractService.java:197) Service org.apache.hadoop.mapred.ResourceMgrDelegate is started
[DEBUG] 2016-09-07 11:25:15.522 [] [] [main] org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction(UserGroupInformation.java:1638) PrivilegedAction as:root (auth:SIMPLE) from:org.apache.hadoop.fs.FileContext.getAbstractFileSystem(FileContext.java:330)
[DEBUG] 2016-09-07 11:25:15.538 [] [] [main] org.apache.hadoop.hdfs.DFSClient$Conf.<init>(DFSClient.java:389) dfs.client.use.legacy.blockreader.local = false
[DEBUG] 2016-09-07 11:25:15.543 [] [] [main] org.apache.hadoop.hdfs.DFSClient$Conf.<init>(DFSClient.java:392) dfs.client.read.shortcircuit = false
[DEBUG] 2016-09-07 11:25:15.544 [] [] [main] org.apache.hadoop.hdfs.DFSClient$Conf.<init>(DFSClient.java:395) dfs.client.domain.socket.data.traffic = false
[DEBUG] 2016-09-07 11:25:15.544 [] [] [main] org.apache.hadoop.hdfs.DFSClient$Conf.<init>(DFSClient.java:398) dfs.domain.socket.path = 
[DEBUG] 2016-09-07 11:25:15.544 [] [] [main] org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy(RetryUtils.java:74) multipleLinearRandomRetry = null
[DEBUG] 2016-09-07 11:25:15.551 [] [] [main] org.apache.hadoop.ipc.ClientCache.getClient(ClientCache.java:63) getting client out of cache: org.apache.hadoop.ipc.Client@1f3c2480
[DEBUG] 2016-09-07 11:25:15.552 [] [] [main] org.apache.hadoop.mapreduce.Cluster.initialize(Cluster.java:103) Picked org.apache.hadoop.mapred.YarnClientProtocolProvider as the ClientProtocolProvider
[DEBUG] 2016-09-07 11:25:15.553 [] [] [main] org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction(UserGroupInformation.java:1638) PrivilegedAction as:root (auth:SIMPLE) from:org.apache.hadoop.mapreduce.Cluster.getFileSystem(Cluster.java:161)
[DEBUG] 2016-09-07 11:25:15.568 [] [] [main] org.apache.hadoop.security.UserGroupInformation.logPrivilegedAction(UserGroupInformation.java:1638) PrivilegedAction as:root (auth:SIMPLE) from:org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
[DEBUG] 2016-09-07 11:25:15.616 [] [] [main] org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:426) The ping interval is 60000 ms.
[DEBUG] 2016-09-07 11:25:15.617 [] [] [main] org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:695) Connecting to HADOOP-NODE1/10.20.0.11:9000
[DEBUG] 2016-09-07 11:25:15.674 [] [] [IPC Client (1671878947) connection to HADOOP-NODE1/10.20.0.11:9000 from root] org.apache.hadoop.ipc.Client$Connection.run(Client.java:945) IPC Client (1671878947) connection to HADOOP-NODE1/10.20.0.11:9000 from root: starting, having connections 1
[DEBUG] 2016-09-07 11:25:15.676 [] [] [IPC Parameter Sending Thread #0] org.apache.hadoop.ipc.Client$Connection$3.run(Client.java:1008) IPC Client (1671878947) connection to HADOOP-NODE1/10.20.0.11:9000 from root sending #0
[DEBUG] 2016-09-07 11:25:15.692 [] [] [IPC Client (1671878947) connection to HADOOP-NODE1/10.20.0.11:9000 from root] org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1065) IPC Client (1671878947) connection to HADOOP-NODE1/10.20.0.11:9000 from root got value #0
[DEBUG] 2016-09-07 11:25:15.698 [] [] [main] org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:221) Call: getFileInfo took 125ms
[DEBUG] 2016-09-07 11:25:25.676 [] [] [IPC Client (1671878947) connection to HADOOP-NODE1/10.20.0.11:9000 from root] org.apache.hadoop.ipc.Client$Connection.close(Client.java:1168) IPC Client (1671878947) connection to HADOOP-NODE1/10.20.0.11:9000 from root: closed
[DEBUG] 2016-09-07 11:25:25.677 [] [] [IPC Client (1671878947) connection to HADOOP-NODE1/10.20.0.11:9000 from root] org.apache.hadoop.ipc.Client$Connection.run(Client.java:963) IPC Client (1671878947) connection to HADOOP-NODE1/10.20.0.11:9000 from root: stopped, remaining connections 0
